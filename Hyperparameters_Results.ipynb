{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZQwY0YVH8GRDrZgqRULXm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"03LNRbY5Pxqz"},"outputs":[],"source":["# Set Hyperparameters\n","SEED=342\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","reps = 1\n","z_dim = 20\n","h_dim = 64\n","epochs = 50\n","batch = 100\n","lr = 0.00001\n","decay = 0.001\n","print_every = 10\n","\n","dataset = IHDP(replications=reps)\n","\n","# Loop for replications\n","for i, (train, valid, test, contfeats, binfeats) in enumerate(dataset.get_train_valid_test()):\n","    print('\\nReplication %i/%i' % (i + 1, reps))\n","    # read out data\n","    (xtr, ttr, ytr), (y_cftr, mu0tr, mu1tr) = train\n","    (xva, tva, yva), (y_cfva, mu0va, mu1va) = valid\n","    (xte, tte, yte), (y_cfte, mu0te, mu1te) = test\n","\n","    # reorder features with binary first and continuous after\n","    perm = binfeats + contfeats\n","    xtr, xva, xte = xtr[:, perm], xva[:, perm], xte[:, perm]\n","    # concatenate train and valid for training\n","    xalltr, talltr, yalltr = np.concatenate([xtr, xva], axis=0), np.concatenate([ttr, tva], axis=0), np.concatenate([ytr, yva], axis=0)\n","\n","    # set evaluator objects\n","    evaluator_train = Evaluator(yalltr, talltr, y_cf=np.concatenate([y_cftr, y_cfva], axis=0),\n","                                mu0=np.concatenate([mu0tr, mu0va], axis=0), mu1=np.concatenate([mu1tr, mu1va], axis=0))\n","    evaluator_test = Evaluator(yte, tte, y_cf=y_cfte, mu0=mu0te, mu1=mu1te)\n","\n","    # zero mean, unit variance for y during training, use ym & ys to correct when using testset\n","    ym, ys = np.mean(ytr), np.std(ytr)\n","    ytr, yva = (ytr - ym) / ys, (yva - ym) / ys\n","\n","    # init networks (overwritten per replication)\n","    x_dim = len(binfeats) + len(contfeats)\n","    p_x_z_dist = p_x_z(dim_in=z_dim, nh=3, dim_h=h_dim, dim_out_bin=len(binfeats), dim_out_con=len(contfeats)).cuda()\n","    p_t_z_dist = p_t_z(dim_in=z_dim, nh=1, dim_h=h_dim, dim_out=1).cuda()\n","    p_y_zt_dist = p_y_zt(dim_in=z_dim, nh=3, dim_h=h_dim, dim_out=1).cuda()\n","    q_t_x_dist = q_t_x(dim_in=x_dim, nh=1, dim_h=h_dim, dim_out=1).cuda()\n","    q_y_xt_dist = q_y_xt(dim_in=x_dim, nh=3, dim_h=h_dim, dim_out=1).cuda()\n","    q_z_tyx_dist = q_z_tyx(dim_in=len(binfeats) + len(contfeats) + 1, nh=3, dim_h=h_dim, dim_out=z_dim).cuda()\n","    p_z_dist = normal.Normal(torch.zeros(z_dim).cuda(), torch.ones(z_dim).cuda())\n","\n","\n","    # Create optimizer\n","    params = list(p_x_z_dist.parameters()) + \\\n","             list(p_t_z_dist.parameters()) + \\\n","             list(p_y_zt_dist.parameters()) + \\\n","             list(q_t_x_dist.parameters()) + \\\n","             list(q_y_xt_dist.parameters()) + \\\n","             list(q_z_tyx_dist.parameters())\n","\n","    # Adam is used, like original implementation, in paper Adamax is suggested\n","    optimizer = optim.Adam(params, lr=lr, weight_decay=decay)\n","\n","    # init q_z inference\n","    q_z_tyx_dist = init_qz(q_z_tyx_dist, p_z_dist, ytr, ttr, xtr)\n","\n","    # set batch size\n","    M = batch\n","\n","    n_epoch, n_iter_per_epoch, idx = epochs, 10 * int(xtr.shape[0] / M), list(range(xtr.shape[0]))\n","\n","    loss = defaultdict(list)\n","    for epoch in range(n_epoch):\n","        # print('Epoch: %i/%i' % (epoch, n_epoch))\n","        loss_sum = 0.\n","        # shuffle index\n","        np.random.shuffle(idx)\n","        # take random batch for training\n","        for j in range(n_iter_per_epoch):\n","            # select random batch\n","            batch = np.random.choice(idx, M)\n","            x_train, y_train, t_train = torch.cuda.FloatTensor(xalltr[batch]), torch.cuda.FloatTensor(yalltr[batch]), \\\n","                                        torch.cuda.FloatTensor(talltr[batch])\n","\n","            # inferred distribution over z\n","            xy = torch.cat((x_train, y_train), 1)\n","            z_infer = q_z_tyx_dist(xy=xy, t=t_train)\n","            # use a single sample to approximate expectation in lowerbound\n","            z_infer_sample = z_infer.sample()\n","\n","            # RECONSTRUCTION LOSS\n","            # p(x|z)\n","            x_bin, x_con = p_x_z_dist(z_infer_sample)\n","            l1 = x_bin.log_prob(x_train[:, :len(binfeats)]).sum(1)\n","            loss['Reconstr_x_bin'].append(l1.sum().cpu().detach().float())\n","            l2 = x_con.log_prob(x_train[:, -len(contfeats):]).sum(1)\n","            loss['Reconstr_x_con'].append(l2.sum().cpu().detach().float())\n","            # p(t|z)\n","            t = p_t_z_dist(z_infer_sample)\n","            l3 = t.log_prob(t_train).squeeze()\n","            loss['Reconstr_t'].append(l3.sum().cpu().detach().float())\n","            # p(y|t,z)\n","            # for training use t_train, in out-of-sample prediction this becomes t_infer\n","            y = p_y_zt_dist(z_infer_sample, t_train)\n","            l4 = y.log_prob(y_train).squeeze()\n","            loss['Reconstr_y'].append(l4.sum().cpu().detach().float())\n","\n","            # REGULARIZATION LOSS\n","            # p(z) - q(z|x,t,y)\n","            # approximate KL\n","            l5 = (p_z_dist.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)).sum(1)\n","            # Analytic KL (seems to make overall performance less stable)\n","            # l5 = (-torch.log(z_infer.stddev) + 1/2*(z_infer.variance + z_infer.mean**2 - 1)).sum(1)\n","            loss['Regularization'].append(l5.sum().cpu().detach().float())\n","\n","            # AUXILIARY LOSS\n","            # q(t|x)\n","            t_infer = q_t_x_dist(x_train)\n","            l6 = t_infer.log_prob(t_train).squeeze()\n","            loss['Auxiliary_t'].append(l6.sum().cpu().detach().float())\n","            # q(y|x,t)\n","            y_infer = q_y_xt_dist(x_train, t_train)\n","            l7 = y_infer.log_prob(y_train).squeeze()\n","            loss['Auxiliary_y'].append(l7.sum().cpu().detach().float())\n","\n","            # Total objective\n","            # inner sum to calculate loss per item, torch.mean over batch\n","            loss_mean = torch.mean(l1 + l2 + l3 + l4 + l5 + l6 + l7)\n","            loss['Total'].append(loss_mean.cpu().detach().numpy())\n","            objective = -loss_mean\n","\n","            optimizer.zero_grad()\n","            # Calculate gradients\n","            objective.backward()\n","            # Update step\n","            optimizer.step()\n","\n","        if epoch >= 10 and (epoch - 10) % print_every == 0:\n","            print('Epoch %i' % epoch)\n","            y0, y1 = get_y0_y1(p_y_zt_dist, q_y_xt_dist, q_z_tyx_dist, torch.tensor(xalltr).cuda(),\n","                               torch.tensor(talltr).cuda())\n","            score_train = evaluator_train.calc_stats(y1, y0)\n","            rmses_train = evaluator_train.y_errors(y0, y1)\n","            print('Training set - ITE: %f, ITE_Error: %f, ATE: %f, ate ATE_Error: %f, PEHE: %f' % score_train)\n","            print('Training set - rmse factual: %f, rmse counterfactual: %f' %rmses_train)\n","\n","            y0, y1 = get_y0_y1(p_y_zt_dist, q_y_xt_dist, q_z_tyx_dist, torch.tensor(xte).cuda(),\n","                               torch.tensor(tte).cuda())\n","            y0, y1 = y0 * ys + ym, y1 * ys + ym\n","            score_test = evaluator_test.calc_stats(y1, y0)\n","            rmses_test = evaluator_test.y_errors(y0, y1)\n","            print('Testset - ITE: %f, ITE_Error: %f, ATE: %f, ATE_Error: %f, PEHE: %f' % score_test)\n","            print('Test set - rmse factual: %f, rmse counterfactual: %f' %rmses_train)\n","\n","    # Save loss plots\n","    for key, value in loss.items():\n","        plt.figure()\n","        plt.plot(np.array(value), label=key)\n","        plt.title(key)\n","        plt.legend()\n","        plt.savefig(f'{key}_loss.png')\n","        plt.close()\n","\n","    # Save y0 and y1 values to text files\n","    np.savetxt('y0_values.txt', y0.flatten())\n","    np.savetxt('y1_values.txt', y1.flatten())\n","\n","    # Plot and save Variational Lower Bound\n","    plt.figure()\n","    plt.plot(np.array(loss['Total']), label='Total')\n","    plt.title('Variational Lower Bound')\n","    plt.legend()\n","    plt.savefig('variational_lower_bound.png')\n","    plt.close()\n","\n","    # Plot and save Actual vs Predicted\n","    plt.figure()\n","    plt.scatter(y0, y1, label='Actual vs Predicted')\n","    plt.xlabel('Actual')\n","    plt.ylabel('Predicted')\n","    plt.title('Actual vs Predicted')\n","    plt.legend()\n","    plt.savefig('actual_vs_predicted.png')\n","    plt.close()\n"]}]}
