{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWGasFMGeGWN/eHGa+gVzS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"n7Vnue6RPfQd"},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, dim_in=25, num_layers=6, dim_model=25, num_heads=8, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","\n","        encoder_layers = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n","\n","        self.linear = nn.Linear(dim_in, dim_model)\n","\n","    def forward(self, x):\n","        x = self.linear(x)  # Project input to the model dimension\n","        x = x.unsqueeze(0)\n","        # print(x.shape)\n","        # x = x.permute(1, 0, 2)  # Transformer expects input in format (seq_len, batch_size, dim_model)\n","        encoded_output = self.transformer_encoder(x)\n","        return encoded_output\n","\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, dim_out=20, num_layers=6, dim_model=20, num_heads=8, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","\n","        decoder_layers = nn.TransformerDecoderLayer(d_model=dim_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers)\n","\n","        self.linear = nn.Linear(dim_model, dim_out)\n","\n","    def forward(self, encoded_output):\n","        # print(encoded_output.shape)\n","        encoded_output = encoded_output.unsqueeze(0)\n","        # print(encoded_output.shape)\n","        decoded_output = self.transformer_decoder(encoded_output, encoded_output)\n","        # print(decoded_output.shape)\n","        decoded_output = decoded_output.squeeze(0) # Back to (batch_size, seq_len, dim_model)\n","        # print(decoded_output.shape)\n","        decoded_output = self.linear(decoded_output)\n","        return decoded_output\n","\n","class p_x_z(nn.Module):\n","\n","    def __init__(self, dim_in=20, nh=3, dim_h=20, dim_out_bin=19, dim_out_con=6):\n","        super().__init__()\n","        # save required vars\n","        self.nh = nh\n","        self.dim_out_bin = dim_out_bin\n","        self.dim_out_con = dim_out_con\n","\n","        # dim_in is dim of latent space z\n","        self.input = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh-1)])\n","        # output layer defined separate for continuous and binary outputs\n","        self.output_bin = nn.Linear(dim_h, dim_out_bin)\n","        # for each output an mu and sigma are estimated\n","        self.output_con_mu = nn.Linear(dim_h, dim_out_con)\n","        self.output_con_sigma = nn.Linear(dim_h, dim_out_con)\n","        self.softplus = nn.Softplus()\n","\n","    def forward(self, z_input):\n","        z = F.elu(self.input(z_input))\n","        for i in range(self.nh-1):\n","            z = F.elu(self.hidden[i](z))\n","        # for binary outputs:\n","        x_bin_p = torch.sigmoid(self.output_bin(z))\n","        x_bin = bernoulli.Bernoulli(x_bin_p)\n","        # for continuous outputs\n","        mu, sigma = self.output_con_mu(z), self.softplus(self.output_con_sigma(z))\n","        x_con = normal.Normal(mu, sigma)\n","\n","        if (z != z).all():\n","            raise ValueError('p(x|z) forward contains NaN')\n","\n","        return x_bin, x_con\n","\n","\n","class p_t_z(nn.Module):\n","\n","    def __init__(self, dim_in=20, nh=1, dim_h=20, dim_out=1):\n","        super().__init__()\n","        # save required vars\n","        self.nh = nh\n","        self.dim_out = dim_out\n","\n","        # dim_in is dim of latent space z\n","        self.input = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh)])\n","        self.output = nn.Linear(dim_h, dim_out)\n","\n","    def forward(self, x):\n","        x = F.elu(self.input(x))\n","        for i in range(self.nh):\n","            x = F.elu(self.hidden[i](x))\n","        # for binary outputs:\n","        out_p = torch.sigmoid(self.output(x))\n","\n","        out = bernoulli.Bernoulli(out_p)\n","        return out\n","\n","\n","class p_y_zt(nn.Module):\n","\n","    def __init__(self, dim_in=20, nh=3, dim_h=20, dim_out=1):\n","        super().__init__()\n","        # save required vars\n","        self.nh = nh\n","        self.dim_out = dim_out\n","\n","        self.decoder = nn.TransformerDecoder(\n","            nn.TransformerDecoderLayer(d_model=dim_in, nhead=4, dim_feedforward=2048, dropout=0.1),\n","            num_layers=6\n","        )\n","\n","        # Separated forwards for different t values, TAR\n","\n","        self.input_t0 = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden_t0 = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh)])\n","        self.mu_t0 = nn.Linear(dim_h, dim_out)\n","\n","        self.input_t1 = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden_t1 = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh)])\n","        self.mu_t1 = nn.Linear(dim_h, dim_out)\n","\n","    def forward(self, z, t):\n","        device = z.device\n","        t = t.to(device)\n","\n","        z = self.decoder(z, z)  # Assuming self-attention, source and target are the same\n","        # Separated forwards for different t values, TAR\n","        x_t0 = F.elu(self.input_t0(z))\n","        for i in range(self.nh):\n","            x_t0 = F.elu(self.hidden_t0[i](x_t0))\n","        mu_t0 = F.elu(self.mu_t0(x_t0))\n","\n","        x_t1 = F.elu(self.input_t1(z))\n","        for i in range(self.nh):\n","            x_t1 = F.elu(self.hidden_t1[i](x_t1))\n","        mu_t1 = F.elu(self.mu_t1(x_t1))\n","        # set mu according to t value\n","        y = normal.Normal((1-t) * mu_t0 + t * mu_t1, 1)\n","        return y\n","####### Inference model / Encoder #######\n","\n","class q_t_x(nn.Module):\n","\n","    def __init__(self, dim_in=25, nh=1, dim_h=20, dim_out=1):\n","        super().__init__()\n","        # save required vars\n","        self.nh = nh\n","        self.dim_out = dim_out\n","        self.encoder = TransformerEncoder(dim_in=dim_in, num_layers=6, dim_model=25, num_heads=5, dim_feedforward=2048, dropout=0.1)\n","        # dim_in is dim of data x\n","        self.input = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh)])\n","        self.output = nn.Linear(dim_h, dim_out)\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = F.elu(self.input(x))\n","        for i in range(self.nh):\n","            x = F.elu(self.hidden[i](x))\n","        # for binary outputs:\n","        out_p = torch.sigmoid(self.output(x))\n","        out = bernoulli.Bernoulli(out_p)\n","\n","        return out\n","\n","\n","class q_y_xt(nn.Module):\n","\n","    def __init__(self, dim_in=25, nh=3, dim_h=20, dim_out=1):\n","        super().__init__()\n","        # save required vars\n","        self.nh = nh\n","        self.dim_out = dim_out\n","\n","        # dim_in is dim of data x\n","        self.input = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh)])\n","        # separate outputs for different values of t\n","        self.mu_t0 = nn.Linear(dim_h, dim_out)\n","        self.mu_t1 = nn.Linear(dim_h, dim_out)\n","\n","    def forward(self, x, t):\n","        # Unlike model network, shared parameters with separated heads\n","        x = F.elu(self.input(x))\n","        for i in range(self.nh):\n","            x = F.elu(self.hidden[i](x))\n","        # only output weights separated\n","        mu_t0 = self.mu_t0(x)\n","        mu_t1 = self.mu_t1(x)\n","        # set mu according to t, sigma set to 1\n","        y = normal.Normal((1-t)*mu_t0 + t * mu_t1, 1)\n","        return y\n","\n","\n","class q_z_tyx(nn.Module):\n","\n","    def __init__(self, dim_in=25+1, nh=3, dim_h=20, dim_out=20):\n","        super().__init__()\n","        # dim in is dim of x + dim of y\n","        # dim_out is dim of latent space z\n","        # save required vars\n","        self.nh = nh\n","\n","        # Shared layers with separated output layers\n","\n","        self.input = nn.Linear(dim_in, dim_h)\n","        # loop through dimensions to create fully con. hidden layers, add params with ModuleList\n","        self.hidden = nn.ModuleList([nn.Linear(dim_h, dim_h) for _ in range(nh)])\n","\n","        self.mu_t0 = nn.Linear(dim_h, dim_out)\n","        self.mu_t1 = nn.Linear(dim_h, dim_out)\n","        self.sigma_t0 = nn.Linear(dim_h, dim_out)\n","        self.sigma_t1 = nn.Linear(dim_h, dim_out)\n","        self.softplus = nn.Softplus()\n","\n","    def forward(self, xy, t):\n","        # Shared layers with separated output layers\n","        # print('before first linear z_infer')\n","        # print(xy)\n","        x = F.elu(self.input(xy))\n","        # print('first linear z_infer')\n","        # print(x)\n","        for i in range(self.nh):\n","            x = F.elu(self.hidden[i](x))\n","\n","        mu_t0 = self.mu_t0(x)\n","        mu_t1 = self.mu_t1(x)\n","        sigma_t0 = self.softplus(self.sigma_t0(x))\n","        sigma_t1 = self.softplus(self.sigma_t1(x))\n","\n","        # Set mu and sigma according to t\n","        z = normal.Normal((1-t)*mu_t0 + t * mu_t1, (1-t)*sigma_t0 + t * sigma_t1)\n","        return z\n","\n","\n"]}]}